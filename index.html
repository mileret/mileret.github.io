<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Zimo He</title>

    <meta name="author" content="Jon Barron">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Zimo He  |  何子默
                </p>
                <p>
                  Welcome to my page!
                </p>
                <p>
                  I'm a first-year Ph.D. student at <a href="https://cs.pku.edu.cn/">PKU School of Computer Science</a>, advised by <a href="https://yzhu.io/">Prof. Yixin Zhu</a> and <a href="https://cfcs.pku.edu.cn/people/faculty/yizhouwang/index.htm">Prof. Yizhou Wang</a>. I'm also a research intern at General Vision Lab, <a href="https://www.bigai.ai/">BIGAI</a>, working on general 3D vision and humanoid robot learning. I earned my Bachelor's degree from <a href="https://yuanpei.pku.edu.cn/xygk/xyjj/index.htm">Yuanpei College</a>, Peking University.
                </p>
                <p style="text-align:center">
                  <a href="milleret.hzm@gmail.com">Email</a> &nbsp;/&nbsp;
                  <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp; -->
                  <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <!-- <a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Scholar</a> &nbsp;/&nbsp; -->
                  <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp; -->
                  <a href="https://github.com/mileret/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:30%;max-width:40%">
                <a href="images/Hezimo.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/Hezimo.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My research interests lie in the intersection of 3D vision, robotics and computer graphics, including human-scene interaction, humanoid robot and human motion synthesis. My long-term goal is to build a general embodied AI that can see, understand and interact with environments, both in virtual and real world.
                </p>
              </td>
            </tr>
          </tbody></table>
          
          <!-- uniact -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding: 20px; width: 18px; vertical-align: top;" class="teaser-cell"><img src="images/uniact.png" ,="" width="165px" class="teaser-image"></td>
              <td style="padding: 15px; width: 80%; vertical-align: top;">
                <a href="https://jnnan.github.io/uniact/">
                  <span class="papertitle">UniAct: Unified Motion Generation and Action Streaming for Humanoid Robots
                  </span>
                </a>
                <br>
                <a href="https://jnnan.github.io/" target="_blank">Nan Jiang</a>*,
                <strong>Zimo He</strong>*,
                <a href="https://pku.ai/author/wanhe-yu/" target="_blank">Wanhe Yu</a>,
                <a href="https://pku.ai/author/lexi-pang/" target="_blank">Lexi Pang</a>,
                <a href="https://pku.ai/author/yunhao-li/" target="_blank">Yunhao Li</a>,
                <a href="https://awfuact.github.io/" target="_blank">Hongjie Li</a>,
                <a href="https://jiemingcui.github.io/" target="_blank">Jieming Cui</a>,
                <a href="" target="_blank">Yuhan Li</a>,
                <a href="https://cfcs.pku.edu.cn/people/faculty/yizhouwang/index.htm" target="_blank">Yizhou Wang</a>,
                <a href="https://yzhu.io/" target="_blank">Yixin Zhu</a>†,
                <a href="https://siyuanhuang.com/" target="_blank">Siyuan Huang</a>†
                <br>
                Under Review
                <br>
                <a href="https://jnnan.github.io/uniact/">project page</a>
                /
                <a href="http://arxiv.org/abs/2512.24321">arXiv</a>
                /
                <a href="https://github.com/jnnan/uniact-code">code</a>
                /
                <a href="https://vimeo.com/1150342107">video</a>
                <p style="display: block;">
                  We introduce a unified framework that enables humanoid robots to execute multimodal instructions by generating motion tokens through a shared representation and achieving real-time whole-body control.
                </p>
              </td>
            </tr>
          </tbody></table>

          <!-- motionediting -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding: 20px; width: 18px; vertical-align: top;" class="teaser-cell"><img src="images/motion_editing.jpg" ,="" width="165px" class="teaser-image"></td>
              <td style="padding: 15px; width: 80%; vertical-align: top;">
                <a href="https://awfuact.github.io/motionrefit/">
                  <span class="papertitle">Dynamic Motion Blending for Versatile Motion Editing
                  </span>
                </a>
                <br>
                <a href="https://jnnan.github.io/" target="_blank">Nan Jiang</a>*,
                <a href="https://awfuact.github.io/">Hongjie Li</a>*,
                <a href="https://pku.ai/author/ziye-yuan/" target="_blank">Ziye Yuan</a>*,
                <strong>Zimo He</strong>,
                <a href="https://yixchen.github.io/" target="_blank">Yixin Chen</a>,
                <a href="https://tengyu.ai/" target="_blank">Tengyu Liu</a>,
                <a href="https://yzhu.io/" target="_blank">Yixin Zhu</a>†,
                <a href="https://siyuanhuang.com/" target="_blank">Siyuan Huang</a>†
                <br>
                <em>CVPR</em>, 2025
                <br>
                <a href="https://awfuact.github.io/motionrefit/">project page</a>
                /
                <a href="https://arxiv.org/abs/2503.20724">arXiv</a>
                /
                <a href="https://github.com/emptybulebox1/motionRefit/">code</a>
                <p style="display: block;">
                  Our text-guided motion editor combines dynamic body-part blending augmentation with an auto-regressive diffusion model to enable diverse motion editing without extensive pre-collected training data.
                </p>
              </td>
            </tr>
          </tbody></table>

          <!-- lingo -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr onmouseout="autonomous_hsi_stop()" onmouseover="autonomous_hsi_start()" bgcolor="">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <!-- <div class="two" id='cat3d_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/cat3d.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div> -->
                  <img src='images/autonomous_hsi.png' width="200", height="120">
                </div>
                <script type="text/javascript">
                  function autonomous_hsi_start() {
                    document.getElementById('autonomous_hsi_image').style.opacity = "1";
                  }

                  function autonomous_hsi_stop() {
                    document.getElementById('autonomous_hsi_image').style.opacity = "0";
                  }
                  autonomous_hsi_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://lingomotions.com/">
                <span class="papertitle">Autonomous Character-Scene Interaction Synthesis from Text Instruction
                </span>
                </a>
                <br>
                <a href="https://jnnan.github.io/">Nan Jiang</a>*,
                <strong>Zimo He</strong>*,
                <a href="">Zi Wang</a>, 
                <a href="https://awfuact.github.io/">Hongjie Li</a>,
                <a href="https://yixchen.github.io/">Yixin Chen</a>, 
                <a href="https://siyuanhuang.com/">Siyuan Huang</a>†, 
                <a href="https://yzhu.io/">Yixin Zhu</a>†

                <br>
                <em>SIGGRAPH Asia</em>, 2024
                <br>
                <a href="https://lingomotions.com/">project page</a>
                /
                <a href="https://arxiv.org/abs/2410.03187">arXiv</a>
                /
                <a href="https://github.com/mileret/lingo-release">code</a>
                <p></p>
                <p>
                  We propose a framework synthesizing multi-stage scene-aware human motions autonomously and directly from the text instruction and goal location. We also introduce LINGO, a comprehensive language-annotated MoCap dataset, featuring Human-Scene Interaction (HSI) motions.
                </p>
              </td>
            </tr>            
            
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Experience</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>

            <tr>
              <td style="padding:10px;width:18px;vertical-align:middle"><img src="images/Institution/pku.jpeg", width="100px"></td>
              <td width="90%" valign="center">
                <b>School of Computer Science, Peking University</b>
                <br> Aug. 2025 - Jul. 2030 (expected)
                <br>
                <br> <b>Ph.D. Student</b>
                <br> Advisor: <a href="http://yzhu.io" target="_blank">Prof. Yixin Zhu</a> and <a href="https://cfcs.pku.edu.cn/people/faculty/yizhouwang/index.htm" target="_blank">Prof. Yizhou Wang</a>
              </td>
            </tr>

            <tr>
              <td style="padding:10px;width:18px;vertical-align:middle"><img src="images/Institution/bigai.png", width="100px"></td>
              <td width="90%" valign="center">
                <b>Beijing Institute for General Artificial Intelligence (BIGAI)</b>
                <br> Apr. 2024 - Present
                <br>
                <br> <b>Research Intern</b>
                <br> Advisor: <a href="https://siyuanhuang.com" target="_blank">Dr. Siyuan Huang</a> and <a href="https://tengyu.ai/" target="_blank">Dr. Tengyu Liu</a>
              </td>
            </tr>

            <tr>
              <td style="padding:10px;width:18px;vertical-align:middle"><img src="images/Institution/core_lab.png", width="100px"></td>
              <td width="90%" valign="center">
                <b>Cognitive Reasoning (CoRe) Lab, Institute for Artificial Intelligence, Peking University</b>
                <br> Jul. 2023 - Present
                <br>
                <br> <b>Research Assistant</b>
                <br> Advisor: <a href="http://yzhu.io" target="_blank">Prof. Yixin Zhu</a>
              </td>
            </tr>

            <tr>
              <td style="padding:10px;width:18px;vertical-align:middle"><img src="images/Institution/pku.jpeg", width="100px"></td>
              <td width="90%" valign="center">
                <b>Tong Class, YuanPei College, Peking University</b>
                <br> Aug. 2021 - Jul. 2025
                <br>
                <br> <b>Undergraduate Student</b>
              </td>
            </tr>
          </tbody></table>


          <table style="width:40%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  This website is a modification to <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's website</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
